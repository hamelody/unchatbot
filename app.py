import streamlit as st
import os
import fitz
import pandas as pd
import docx
from pptx import Presentation
import faiss
import openai
import numpy as np
import json
import io
from datetime import datetime
from openai import AzureOpenAI
from azure.storage.blob import BlobServiceClient

# --- ê¸°ë³¸ ì„¸íŒ… ---
st.set_page_config(page_title="ìœ ì•¤ìƒëª…ê³¼í•™ ì—…ë¬´ ê°€ì´ë“œ ë´‡", layout="centered")

# Azure OpenAI Client ì´ˆê¸°í™”
openai_client = AzureOpenAI(
    api_key=st.secrets["AZURE_OPENAI_KEY"],
    azure_endpoint=st.secrets["AZURE_OPENAI_ENDPOINT"],
    api_version=st.secrets["AZURE_OPENAI_VERSION"]
)

# Azure Blob Storage ì´ˆê¸°í™”
blob_service = BlobServiceClient.from_connection_string(st.secrets["AZURE_BLOB_CONN"])
container_client = blob_service.get_container_client(st.secrets["BLOB_CONTAINER"])

def save_to_blob(uploaded_file):
    data = uploaded_file.getvalue()
    blob_client = container_client.get_blob_client(uploaded_file.name)
    blob_client.upload_blob(data, overwrite=True)
    return f"https://{blob_service.account_name}.blob.core.windows.net/{container_client.container_name}/{uploaded_file.name}"

# íŒŒì¼ ê²½ë¡œ ì„¤ì •
INDEX_PATH = "vector.index"
LOG_PATH = "upload_log.json"
USAGE_LOG_PATH = "usage_log.json"
RULES_PATH = ".streamlit/prompt_rules.txt"

# ì„¤ì •
EMBEDDING_MODEL = st.secrets["AZURE_OPENAI_EMBEDDING_DEPLOYMENT"]
ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
TOKEN_COST = float(st.secrets.get("TOKEN_COST", 0.0))
CHUNK_SIZE = 500

# ë²¡í„°DB ì´ˆê¸°í™”
if os.path.exists(INDEX_PATH):
    index = faiss.read_index(INDEX_PATH)
    with open("metadata.json", "r", encoding="utf-8") as f:
        metadata = json.load(f)
else:
    index = faiss.IndexFlatL2(1536)
    metadata = []

# ê·œì¹™ ë¡œë“œ
def load_rules():
    if os.path.exists(RULES_PATH):
        return open(RULES_PATH, encoding="utf-8").read()
    return "ë‹¹ì‹ ì€ ì œì•½íšŒì‚¬ DI/GMP ì „ë¬¸ê°€ ì±—ë´‡ì…ë‹ˆë‹¤."

# ë¬¸ì„œ íŒŒì‹±
def extract_text(file):
    ext = os.path.splitext(file.name)[1].lower()
    if ext == '.pdf':
        doc = fitz.open(stream=file.read(), filetype="pdf")
        return "\n".join(page.get_text() for page in doc)
    if ext == '.docx':
        doc_obj = docx.Document(file)
        return "\n".join(p.text for p in doc_obj.paragraphs)
    if ext in ('.xlsx', '.xlsm'):
        df = pd.read_excel(file)
        return df.to_string(index=False)
    if ext == '.csv':
        df = pd.read_csv(file)
        return df.to_string(index=False)
    if ext == '.pptx':
        prs = Presentation(file)
        text = ""
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    text += shape.text + "\n"
        return text
    return ""

# í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• 
def chunk_text(text):
    chunks, buf = [], ""
    for line in text.split("\n"):
        if len(buf) + len(line) < CHUNK_SIZE:
            buf += line + "\n"
        else:
            chunks.append(buf.strip())
            buf = line + "\n"
    if buf:
        chunks.append(buf.strip())
    return chunks

# ì„ë² ë”© í˜¸ì¶œ
def get_embedding(text):
    resp = openai_client.embeddings.create(input=[text], model=EMBEDDING_MODEL)
    return resp.data[0].embedding

# ë¬¸ì„œ DB ì¶”ê°€
def add_document(file, text, chunks):
    import numpy as np
    vecs, metas = [], []
    for c in chunks:
        emb = get_embedding(c)
        vecs.append(emb)
        metas.append({"file_name": file.name, "content": c})
    index.add(np.array(vecs).astype("float32"))
    metadata.extend(metas)
    faiss.write_index(index, INDEX_PATH)
    with open("metadata.json", "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log = {"file": file.name, "time": now, "chunks": len(chunks)}
    logs = json.load(open(LOG_PATH, encoding="utf-8")) if os.path.exists(LOG_PATH) else []
    logs.append(log)
    json.dump(logs, open(LOG_PATH, "w", encoding="utf-8"), ensure_ascii=False, indent=2)

# ë¬¸ì„œ ê²€ìƒ‰
def search_similar(query_or_chunk, k=5):
    qv = get_embedding(query_or_chunk)
    if index.ntotal == 0 or not metadata:
        return []
    D, I = index.search(np.array([qv]).astype("float32"), k)
    return [metadata[i]["content"] for i in I[0] if i < len(metadata)]

# --- UI ì‹œì‘ ---
st.markdown("""
<div style="text-align:center; margin-bottom:24px;">
  <span style="font-size:2.1rem; font-weight:bold;">ìœ ì•¤ìƒëª…ê³¼í•™ GMP/SOP ì—…ë¬´ ê°€ì´ë“œ ë´‡</span>
  <span style="font-size:1rem; color:gray; margin-left:8px;">ver 0.5</span>
</div>
""", unsafe_allow_html=True)

# íƒ­ ìƒì„±
tab2, tab1 = st.tabs(["ğŸ’¬ ì—…ë¬´ ì§ˆë¬¸", "âš™ï¸ ê´€ë¦¬ì ì„¤ì •"])

# --- ì—…ë¬´ ì§ˆë¬¸ íƒ­ ---
with tab2:
    st.header("ì—…ë¬´ ì§ˆë¬¸")
    st.markdown("""
    <div style='background-color:#edf4fb;padding:18px;border-radius:10px; margin-bottom:12px; max-width:700px; margin:auto;'>
      ğŸ’¡ ì˜ˆì‹œ: SOP ë°±ì—… ì£¼ê¸°, PIC/S Annex 11 ì°¨ì´ ë“±
    </div>
    """, unsafe_allow_html=True)
    with st.form(key='query_form'):
        file_u = st.file_uploader("íŒŒì¼ ì²¨ë¶€ (ì„ íƒ)", type=["pdf","docx","xlsx","xlsm","csv","pptx"])
        q = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...", key="input_text")
        submit = st.form_submit_button("ì „ì†¡")
    if submit and (q.strip() or file_u):
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            rules = load_rules()
            ctx = []
            if file_u:
                text = extract_text(file_u)
                chunks = chunk_text(text)
                for chunk in chunks:
                    ctx.extend(search_similar(chunk))
            else:
                ctx = search_similar(q)
            prompt = f"{rules}\n\nì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ ë‹µë³€í•˜ì„¸ìš”:\n" + "\n\n".join(ctx)
            resp = openai_client.chat.completions.create(
                model=st.secrets["AZURE_OPENAI_DEPLOYMENT"],
                messages=[{"role":"system","content":prompt},{"role":"user","content":q}],
                max_tokens=1000,
                temperature=0.2,
            )
            answer = resp.choices[0].message.content
            now = datetime.now().strftime("%Y-%m-%d %H:%M")
            st.markdown(f"""
            <div style='max-width:700px; margin:auto;'>
              <div style='text-align:right; margin-bottom:8px;'>
                <div style='display:inline-block; background-color:#f1f3f5; padding:10px; border-radius:15px; border-bottom-right-radius:0; max-width:80%;'>
                  <div style="font-size:12px; color:gray; text-align:right;">{now}</div>
                  {q}
                </div>
              </div>
              <div style='text-align:left;'>
                <div style='display:inline-block; background-color:#d4edda; padding:10px; border-radius:15px; border-bottom-left-radius:0; max-width:80%;'>
                  <div style="font-size:12px; color:gray; text-align:left;">{now}</div>
                  {answer}
                </div>
              </div>
            </div>
            """, unsafe_allow_html=True)
            st.session_state.last_q = q

# --- ê´€ë¦¬ì ì„¤ì • íƒ­ ---
with tab1:
    st.header("âš™ï¸ ê´€ë¦¬ì ì„¤ì •")
    pw = st.text_input("ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸", type="password")
    if pw == ADMIN_PASSWORD:
        f = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ (í•™ìŠµìš©)", type=["pdf","docx","xlsx","xlsm","csv","pptx"])
        if f:
            with st.spinner("ì²˜ë¦¬ ì¤‘..."):
                blob_url = save_to_blob(f)
                text = extract_text(f)
                chunks = chunk_text(text)
                add_document(f, text, chunks)
                st.success(f"âœ… {f.name} ì—…ë¡œë“œ ë° í•™ìŠµ ì™„ë£Œ (Blob URL: {blob_url})")

        st.markdown("---")
        with st.expander("ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"):
            if os.path.exists(USAGE_LOG_PATH):
                use_logs = json.load(open(USAGE_LOG_PATH, encoding="utf-8"))
                df_use = pd.DataFrame(use_logs)
                total_calls = len(df_use)
                total_tokens = df_use["total_tokens"].sum()
                avg_tokens = total_tokens / total_calls if total_calls else 0
                total_cost = total_tokens * TOKEN_COST
                st.write(f"ì´ í˜¸ì¶œ íšŸìˆ˜: {total_calls}")
                st.write(f"ì´ í† í° ì‚¬ìš©ëŸ‰: {total_tokens}")
                st.write(f"í‰ê·  í† í° ì‚¬ìš©ëŸ‰: {avg_tokens:.1f}")
                st.write(f"ì˜ˆìƒ ë¹„ìš© (USD): {total_cost:.4f}")
                st.dataframe(df_use)
            else:
                st.write("ì•„ì§ í˜¸ì¶œ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

        st.markdown("---")
        st.subheader("Blobì— ì €ì¥ëœ íŒŒì¼ ëª©ë¡")
        blobs = container_client.list_blobs()
        blob_list = [{"íŒŒì¼ëª…": b.name, "ìµœì¢… ìˆ˜ì •": b.last_modified} for b in blobs]
        if blob_list:
            df_blob = pd.DataFrame(blob_list)
            st.dataframe(df_blob)
        else:
            st.write("Blob ì»¨í…Œì´ë„ˆì— ì €ì¥ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning("ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")